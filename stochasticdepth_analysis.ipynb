{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the \"Concrete Crack Images for Classification\" Dataset first below is the link to the dataset\n",
    "https://data.mendeley.com/datasets/5y9wdsg2zt/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first split the dataset into train val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "##the data path should be the path to the folder where the imeediate subfolders are the classes\n",
    "data_path = \"data/Concrete_Crack_Images_for_Classification\"\n",
    "\n",
    "train_path = data_path+\"/train\"\n",
    "val_path = data_path+\"/val\"\n",
    "test_path = data_path+\"/test\"\n",
    "\n",
    "#if you are running it again we dont need to split the data so this is just the check \n",
    "data_path_paths = glob.glob(data_path+'/**/*.jpg',recursive=True)\n",
    "\n",
    "if len(data_path_paths)>0:\n",
    "    #we will make a 80% 10% 10% split\n",
    "    #######################################################\n",
    "    for clss in os.listdir(data_path):\n",
    "        img_paths = glob.glob(data_path+'/'+clss+\"/**/*.jpg\",recursive=True)\n",
    "        random.shuffle(img_paths)\n",
    "        os.makedirs(train_path+'/'+clss,exist_ok=True)\n",
    "        for img_p in img_paths[:int(len(img_paths)*0.80)]:\n",
    "            shutil.move(img_p,train_path+'/'+clss)\n",
    "        os.makedirs(val_path+'/'+clss,exist_ok=True)\n",
    "        for img_p in img_paths[int(len(img_paths)*0.80):int(len(img_paths)*0.90)]:\n",
    "            shutil.move(img_p,val_path+'/'+clss)\n",
    "        os.makedirs(test_path+'/'+clss,exist_ok=True)\n",
    "        for img_p in img_paths[int(len(img_paths)*0.90):]:\n",
    "            shutil.move(img_p,test_path+'/'+clss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "#for the transforms we will not be resizing the images since this dataset already has resized images\n",
    "\n",
    "transforms = torchvision.transforms.ToTensor()\n",
    "concrete_train_dataset = torchvision.datasets.ImageFolder(train_path,transform=transforms)\n",
    "concrete_val_dataset = torchvision.datasets.ImageFolder(val_path,transform=transforms)\n",
    "concrete_test_dataset = torchvision.datasets.ImageFolder(test_path,transform=transforms)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
